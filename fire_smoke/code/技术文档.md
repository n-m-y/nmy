# 烟雾火灾检测系统项目技术文档


## 1.项目概述
### 1.1 项目背景

本项目基于先进的计算机视觉与深度学习技术，构建了一个集**火焰与烟雾检测、历史记录管理、智能问答与数据统计分析**于一体的综合性火灾预警系统。

系统核心采用**YOLOv8目标检测模型**，实现对图像与视频中火焰、烟雾的快速准确识别。后端采用**FastAPI**框架，前端通过简洁交互界面实现用户操作，整体系统具备良好的响应性与可扩展性。

### 1.2 项目目标
- 实现火焰和烟雾的高效检测
- 提供用户友好的交互界面
- 支持图片和视频的上传与检测
- 提供实时视频流检测功能
- 提供检测结果的可视化展示
- 提供ai对话交流
- 提供历史测试记录查询
- 对管理员提供数据统计面板
------

## 2.系统框架
### 2.1 后端框架
后端基于FastAPI构建，提供以下主要接口：
- 图片检测：POST /detect，接收上传的图片并返回检测结果
- 视频检测：POST /detect_video，接收上传的视频并返回处理后的视频（速度较慢）
- WebSocket实时视频检测：/ws/video-detection，接收实时视频帧并返回检测结果（速度快）

### 2.2 前端构架
前端基于HTML/CSS/JavaScript构建，主要功能包括：
- 文件上传和预览
- 检测结果展示
- 实时视频流处理
- 语音提示功能
- ai问答 
- 数据看板
-  历史记录统计

### 2.3 项目结构
```bat
fun/
├── static/ 
│    ├── results/ 
│    ├── script.js/
│    └── styles.css/ 
├── templates/ 
│    └── index.html/ 
├── uploads/ 
├── app.py
├── best.pt 
├── detection_logs.json 
├── qa_history.json 
├── openh264-1.8.0-win64.dll 
├── README.md
└── requirements.txt
```
------

## 3.功能模块
### 3.1 图片检测

   用户上传图片后，系统通过YOLOv8模型进行火焰和烟雾检测，并返回检测结果的可视化图片。

   1.用户上传图片

   2.后端接收图片并进行预处理

   3.使用YOLOv8模型进行目标检测

   4.将检测结果绘制到图片上

   5.返回处理后的图片和检测结果

### 3.2 实时视频流检测

通过WebSocket连接，用户可以实时上传视频帧并获取检测结果。

   1.前端通过WebSocket连接后端

   2.用户上传实时视频帧

   3.后端对每帧进行目标检测

   4.返回检测结果和处理后的图片

   5.前端展示处理后的图片并提供语音提示

### 3.3 视频检测

用户上传视频后，系统逐帧处理视频并返回处理后的视频文件。

1.用户上传视频

2.后端逐帧读取视频

3.每帧使用YOLOv8模型进行目标检测

4.将检测结果绘制到视频帧上

5.合并处理后的视频帧并返回处理后的视频文件

### **3.4** 历史记录查询

用户使用系统后，可以查询自己的使用记录

### **3.5AI助手对话**

用户可以向DeepSeek微调模型进行相关的内容对话

**3.6数据看板统计**

管理员输入权限密钥后可以查看数据看板对于查询结果的统计和所绘制的趋势图

-----

## 4.技术实现

### 4.1 YOLOv8模型
系统使用YOLOv8模型进行目标检测，模型文件路径为best.pt。模型加载代码如下：
```python
from ultralytics import YOLO
model = YOLO("best.pt")
```
模型训练代码如下：

```python
results = model.train(
        data='data.yaml',
        epochs=100,
        imgsz=640,
        batch=8,
        name='FIRE',
        workers=4
)
```
数据集地址：https://www.kaggle.com/datasets/sayedgamal99/smoke-fire-detection-yolo

### **4.2 DeepSeek微调模型**

系统集成了基于DeepSeek-R1的LoRA微调模型， 支持消防安全相关知识的自然语言交互问答。

```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None
)
```

- 使用 `FastLanguageModel.get_peft_model` 将基础模型转换为 LoRA 微调模型。
- 设置 LoRA 的关键参数：
  - `r=16`：LoRA 矩阵的秩，决定可训练参数的数量。
  - `target_modules`：指定哪些模块应用 LoRA，通常是 Transformer 架构中的投影层。
  - `lora_alpha` 和 `lora_dropout`：控制 LoRA 的缩放因子和 dropout 率。
  - `use_gradient_checkpointing`：启用梯度检查点技术，优化内存使用。

```python
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=600,
        learning_rate=2e-4,
        optim="adamw_8bit",
        weight_decay=0.01,
        fp16=True,
        logging_steps=10,
        output_dir="outputs"
    )
)
trainer.train()
```

- 使用 `SFTTrainer`（监督式微调训练器）进行训练。
- 配置训练参数：
  - `per_device_train_batch_size`：每个设备的批次大小。
  - `gradient_accumulation_steps`：梯度累积步数。
  - `learning_rate`：学习率。
  - `optim`：优化器（8 位 AdamW）。
  - `fp16`：使用半精度训练以节省显存

### 4.3 图片检测实现

1.读取上传的图片

2.使用YOLOv8模型进行检测

3.绘制检测框并标注类别

4.返回处理后的图片和检测结果

```python
@app.post("/detect")
async def detect(file: UploadFile = File(...)):
    # 读取图片
    image = cv2.imread(temp_filename)
    results = model(image)
    # 绘制检测框
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            class_name = model.names.get(cls_id, "未知")
            draw.rectangle([int(x1), int(y1), int(x2), int(y2)], outline="green", width=2)
    # 返回结果
    return JSONResponse(content=jsonable_encoder(response_data))
```
### 4.4 视频检测实现

1.读取上传的视频

2.逐帧处理视频

3.使用YOLOv8模型进行检测

4.将检测结果绘制到视频帧上

5.合并处理后的视频帧并返回

```python
@app.post("/detect_video")
async def detect_video(file: UploadFile = File(...)):
    cap = cv2.VideoCapture(temp_filename)
    out = cv2.VideoWriter(result_filename, fourcc, fps, (width, height))
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        results = model(frame)
        # 绘制检测框
        for result in results:
            boxes = result.boxes
            for box in boxes:
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                class_name = model.names.get(cls_id, "未知")
                draw.rectangle([int(x1), int(y1), int(x2), int(y2)], outline="green", width=2)
        frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)
        out.write(frame)
    return StreamingResponse(open(result_filename, "rb"), media_type="video/mp4")
```

### 4.5 实时视频流检测实现

1.前端通过WebSocket连接后端

2.前端上传实时视频帧

3.后端对每帧进行目标检测

4.返回检测结果和处理后的图片

5.前端展示处理后的图片并提供语音提示

```python
@app.websocket("/ws/video-detection")
async def websocket_video_detection(websocket: WebSocket):
    await websocket.accept()
    while True:
        data = await websocket.receive_bytes()
        frame = cv2.imdecode(np.frombuffer(data, np.uint8), cv2.IMREAD_COLOR)
        results = model(frame)
        processed_frame = results[0].plot()
        _, buffer = cv2.imencode('.jpg', processed_frame)
        processed_image_base64 = base64.b64encode(buffer).decode('utf-8')
        await websocket.send_json({
            "type": "detection",
            "classes": list(detected_classes),
            "processedImage": processed_image_base64,
            "timestamp": time.time()
        })
```

### 4.6 数据看板查询

1.图片/视频检测保存date数据到文件中

2.前端导入后端传来的数据

3.数据看板统计

```python
@app.get("/dashboard")
async def dashboard():
    data = generate_dashboard_data()
    return data
```

### 4.7 历史记录查询

1. 问答历史记录分页查询

（1）前端传递分页参数（页码、每页数量）

（2）后端从数据库/文件加载历史记录

（3）计算分页数据并返回结构化结果

2. 检测结果文件查询

（1）扫描服务器结果目录

（2）过滤有效媒体文件（图片/视频）

（3）提取文件元数据（类型、检测标签等）

（4）分页返回排序后的结果


```python
@app.get("/getresults")
async def get_results(page: int = 1, size: int = 10):
    results_dir = "static/results"
    if not os.path.exists(results_dir):
        return []

    files = []
    for filename in os.listdir(results_dir):
        file_path = os.path.join(results_dir, filename)
        if os.path.isfile(file_path):
            # 判断文件类型
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_type = "image"
            elif filename.lower().endswith(('.mp4', '.avi', '.mov')):
                file_type = "video"
            else:
                continue

            file_date = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(file_path)))
            files.append({
                "name": filename,
                "url": f"/static/results/{filename}",
                "type": file_type,
                "date": file_date,
                "mime": "video/mp4" if file_type == "video" else "image/jpeg",
                "is_fire": "fire" in filename.lower(),
                "is_smoke": "smoke" in filename.lower()
            })

@app.get("/qa-history")
async def get_qa_history(page: int = 1, size: int = 10):
    history = load_history()
    total_records = len(history)
    total_pages = (total_records + size - 1) // size
    current_page = max(1, min(page, total_pages))
    start = (current_page - 1) * size
    end = min(start + size, total_records)
    records = history[start:end]

    return {
        "currentPage": current_page,
        "totalPages": total_pages,
        "totalRecords": total_records,
        "prevPage": current_page - 1 if current_page > 1 else None,
        "nextPage": current_page + 1 if current_page < total_pages else None,
        "records": records
    }
```
### 4.8 ai问答

1.接收用户提问文本

2.调用AI生成模型获取原始响应

3.处理响应内容（提取/过滤/格式化）

4.保存问答记录到历史

5.返回结构化答案

```python
@app.post("/ask")
async def ask_question(payload: dict):
    question = payload.get("question", "")

    if not question:
        raise HTTPException(status_code=400, detail="问题不能为空")

    try:

        import re

        # 模拟调用生成模型
        response = generator(question, max_length=100, num_return_sequences=1)

        # 原始生成内容
        generated = response[0]["generated_text"]

        # 提取回答部分（### Response: 后的内容）
        if "### Response:" in generated:
            answer = generated.split("### Response:")[1].strip()
        else:
            answer = generated.strip()

        # 清理文本中的英文和不相关内容（如果有的话）
        answer = re.sub(r'[a-zA-Z]+', '', answer)  # 移除所有英文字符

        # 提取 think 内容
        if "<think>" in answer and "</think>" in answer:
            think = answer.split("<think>")[1].split("</think>")[0].strip()

        # 提取 think 之后的最终回答部分
        if "</think>" in answer:
            final_response = answer.split("</think>")[1].strip()
        else:
            final_response = answer  # 如果没有 <think> 标签，就直接当成最终回答
        final_response = re.sub(r'[<>,/,*,:]', '', final_response)
        # 将内容拼接为一块，使用换行符分隔各部分
        final_answer = f"{final_response}"


        # 创建历史记录条目
        history_entry = {
            "question": question,
            "answer": final_answer,
            "timestamp": datetime.utcnow().isoformat()
        }

        # 添加到历史记录
        qa_history.append(history_entry)
        save_history(history_entry)


        # 返回结构化的 answer 字段
        return {
            "answer": final_answer
        }

    except Exception as e:
        return {"answer": f"发生错误: {e}"}
```

------




## 5.安装与部署

1.安装Anaconda

进入Anaconda的官网（https://www.anaconda.com/products/individual#macos），再根据自己的操作系统选择相应的选项下载软件。

2.下载项目:[nmy/fire_smoke/code/fun at master · n-m-y/nmy](https://github.com/n-m-y/nmy/tree/master/fire_smoke/code/fun)

3.解压

4.创建虚拟环境

```bash
# 创建虚拟环境
conda create -n yolo_env python==3.10
# 激活环境 conda env list 查看所有的虚拟环境
conda activate yolo_env
```

5.用PyCharm打开项目，并设置python环境为上面的虚拟环境

![1](https://raw.githubusercontent.com/n-m-y/nmy/master/fire_smoke/png/1.png)

6.安装项目所需依赖
   pip install -r requirements.txt

7.在hugging face上下载DeekSeek微调模型[gfdgdfasfsf/DeepSeek-R1-fire-1 at main](https://huggingface.co/gfdgdfasfsf/DeepSeek-R1-fire-1/tree/main)

8.更改模型路径

![2](https://raw.githubusercontent.com/n-m-y/nmy/master/fire_smoke/png/2.png)

9.启动 FastAPI 后端
默认运行在 `http://127.0.0.1:5000/`

10.访问前端页面

------

## 6.使用方法

1.打开网页。

2.选择图片检测或视频检测

3.上传一张图片/视频。

4.点击“开始检测”按钮。

5.系统返回检测结果。

6.使用ai问答功能。

7.查看数据看板、预警趋势图。

8.查看历史检测记录。

------

## 7.常见问题与解决方法

### 7.1 图片上传失败
- 可能原因：文件类型不支持或文件过大
- 解决方法：确保上传的图片为JPG、PNG或JPEG格式，且文件大小不超过10MB

### 7.2 视频处理失败
- 可能原因：视频格式不支持或文件损坏
- 解决方法：确保上传的视频为MP4、AVI或MOV格式，且文件大小不超过100MB

### 7.3 WebSocket连接失败
- 可能原因：网络不稳定或浏览器不支持WebSocket
- 解决方法：检查网络连接，或切换到传统视频检测模式

-------

## 8. 未来改进方向
- 提供云端部署支持
- 增加更多目标类别（如人员、设备等）
- 优化模型性能，提高检测速度和准确性
- 增加移动端支持

-----

### 9. 联系方式
如有任何问题或建议，请联系：
- Github：https://github.com/n-m-y